{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "055b86ef-3f74-4584-b6fb-54c14f7c597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, MT5ForConditionalGeneration, Trainer, TrainingArguments, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset, concatenate_datasets, ClassLabel, Dataset\n",
    "import numpy as np\n",
    "import huggingface_hub\n",
    "from sklearn.metrics import f1_score\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "token = 'Nope'\n",
    "huggingface_hub.login(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd2e25e0-16a9-4564-88f1-444d8eb80b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_language_column(dataset, language):\n",
    "    # lang_map = {'en': 0, 'de': 1, 'am': 2, 'es': 3, 'ru': 4, 'zh': 5, 'ar': 6, 'uk': 7, 'hi': 8}\n",
    "    return dataset.add_column(\"Language\", [language] * len(dataset))\n",
    "\n",
    "dataset = load_dataset('textdetox/multilingual_toxic_spans')\n",
    "\n",
    "datasets_with_language = []\n",
    "for lang, data in dataset.items():\n",
    "    dataset_with_language = add_language_column(data, lang)\n",
    "    datasets_with_language.append(dataset_with_language)\n",
    "\n",
    "toxic_spans = concatenate_datasets(datasets_with_language)\n",
    "\n",
    "toxic_spans = toxic_spans.filter(lambda example: example['Negative Connotations'] is not None)\n",
    "\n",
    "synthdetoxm = load_dataset('s-nlp/synthdetoxm')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e076e1c9-d249-464d-9662-71d1cf471de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df8163ac0854e3895fa3846ad03b147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8729 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def just_tokenize(examples):\n",
    "    tokenized_input = tokenizer([examples['Sentence']], padding='max_length', max_length=128, return_tensors=\"pt\", is_split_into_words=True)\n",
    "    \n",
    "    word_tokens = tokenized_input.tokens()\n",
    "    word_ids = tokenized_input.word_ids()\n",
    "\n",
    "    toxic_tokens = tokenizer([examples['Negative Connotations'].replace(',', ' ')], padding='max_length', max_length=128, return_tensors=\"pt\", is_split_into_words=True).tokens()\n",
    "\n",
    "    return {'Word tokens': word_tokens, 'Toxic tokens': toxic_tokens, 'input_ids': tokenized_input['input_ids'], 'attention_mask': tokenized_input['attention_mask']}\n",
    "\n",
    "def align_labels_with_tokens(example):\n",
    "    word_tokens = example['Word tokens']\n",
    "    toxic_something = example['Toxic tokens']\n",
    "    toxic_tokens = []\n",
    "    for tk in toxic_something:\n",
    "        if tk not in tokenizer.all_special_tokens:\n",
    "            toxic_tokens.append(tk)\n",
    "    \n",
    "    token_classes = []\n",
    "    for wt in word_tokens:\n",
    "        if wt in toxic_tokens and wt!='▁' and wt!='' and wt!=' ':\n",
    "            token_classes.append(1)\n",
    "        else:\n",
    "            token_classes.append(0)\n",
    "\n",
    "    return {'labels': token_classes}\n",
    "\n",
    "\n",
    "def go_squeeze(examples):\n",
    "    input_ids = examples['input_ids'].squeeze()\n",
    "    attention_mask = examples['attention_mask'].squeeze()\n",
    "    labels = examples['labels'].squeeze()\n",
    "\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "toxic_spans = toxic_spans.map(\n",
    "    just_tokenize,\n",
    "    batched=False,\n",
    ")\n",
    "\n",
    "toxic_spans = toxic_spans.map(\n",
    "    align_labels_with_tokens,\n",
    "    batched=False,\n",
    ")\n",
    "\n",
    "toxic_spans.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "toxic_spans = toxic_spans.map(\n",
    "    go_squeeze,\n",
    "    batched=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "d3ab5377-aba6-40ef-8744-03ed5798191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toxic_spans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "17cf91c7-4296-4f24-965c-7143cd30a127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64910f0a986343c7a726a912f3b2ab25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    for tox, neu, lang in zip(examples['toxic_sentence'], examples['neutral_sentence'], examples['lang']):\n",
    "        lang_map = {\n",
    "            'de': 'german',\n",
    "            'fr': 'french',\n",
    "            'es': 'spanish',\n",
    "            'ru': 'russian'\n",
    "        }\n",
    "        if tox:  # If toxic text is not empty\n",
    "            inputs.append(\"Detoxify and return answer in \" + lang_map[lang] + \": \" + tox)\n",
    "            targets.append(neu)\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    \n",
    "    # Tokenize target texts\n",
    "    labels = tokenizer(targets, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Preprocess the dataset\n",
    "# train_test = dataset['train'].train_test_split(test_size=0.2, shuffle=True, seed=66)\n",
    "# train_dataset = train_test['train']\n",
    "# val_dataset = train_test['test']\n",
    "\n",
    "synthdetoxm = synthdetoxm.map(preprocess_function, batched=True)\n",
    "\n",
    "synthdetoxm.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "b346a57a-eebb-48f4-b484-503dbb92b3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Sentence', 'Negative Connotations', 'Language', 'Word tokens', 'Toxic tokens', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 8729\n",
       "})"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "92c4015b-75f4-4a98-a2a4-bf4dd5d113a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['toxic_sentence', 'neutral_sentence', 'lang', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthdetoxm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "0db23857-bc76-4ea4-ab93-b25888af33b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of TwoLossesModel were not initialized from the model checkpoint at bigscience/mt0-base and are newly initialized: ['classification_head.bias', 'classification_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bigscience/mt0-base')\n",
    "\n",
    "class TwoLossesModel(MT5ForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.classification_head = nn.Linear(config.d_model, 2)  # Assuming binary classification\n",
    "\n",
    "model = TwoLossesModel.from_pretrained(\"bigscience/mt0-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "d08be7b7-cc9e-4f1d-a98d-de30139d2a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_collator = Dataset.from_dict({'cls_input_ids':toxic_spans['input_ids'][:8000], \n",
    "                                     'cls_attention_mask': toxic_spans['attention_mask'][:8000], \n",
    "                                     'cls_labels': synthdetoxm['labels'][:8000], \n",
    "                                     'detox_input_ids': synthdetoxm['input_ids'][:8000], \n",
    "                                     'detox_attention_mask': synthdetoxm['attention_mask'][:8000],\n",
    "                                     'detox_labels': synthdetoxm['labels'][:8000]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a0bc3d86-8212-4239-85ba-d2f96be4c664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(toxic_spans_data, synthdetoxm):\n",
    "    df_for_collator = pd.DataFrame(np.array([toxic_spans_data['Sentence'][:8000], \n",
    "                                         toxic_spans_data['Negative Connotations'][:8000], \n",
    "                                         synthdetoxm['toxic_sentence'][:8000], \n",
    "                                         synthdetoxm['neutral_sentence'][:8000], \n",
    "                                         synthdetoxm['lang'][:8000]]).T,\n",
    "                               columns=['cls_data', 'cls_labels', 'detox_data', 'detox_labels', 'detox_lang'])\n",
    "    return Dataset.from_pandas(df_for_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "dc2f6e71-0996-46eb-b5d9-0342b6eb60dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_prepared = prepare_dataset(toxic_spans_data, synthdetoxm)\n",
    "\n",
    "train_test = df_for_collator.train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
    "train_dataset = train_test['train']\n",
    "eval_dataset = train_test['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "865edb01-613c-4d6e-b7ff-585929463d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(type='torch', columns=['cls_input_ids', 'cls_attention_mask', 'cls_labels', 'detox_input_ids', 'detox_attention_mask', 'detox_labels'])\n",
    "eval_dataset.set_format(type='torch', columns=['cls_input_ids', 'cls_attention_mask', 'cls_labels', 'detox_input_ids', 'detox_attention_mask', 'detox_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "5c960dc7-550f-485e-a020-19266e101767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Define your classification loss\n",
    "        self.classification_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # Unpack inputs\n",
    "        cls_input_ids = inputs.get(\"cls_input_ids\")\n",
    "        cls_attention_mask = inputs.get(\"cls_attention_mask\")\n",
    "        cls_labels = inputs.get(\"cls_labels\")\n",
    "        detox_input_ids = inputs.get(\"detox_input_ids\")\n",
    "        detox_attention_mask = inputs.get(\"detox_attention_mask\")\n",
    "        detox_labels = inputs.get(\"detox_labels\")\n",
    "\n",
    "        # for i in [cls_input_ids, cls_attention_mask, cls_labels, detox_input_ids, detox_attention_mask, detox_labels]:\n",
    "        #     print(i.shape)\n",
    "\n",
    "        # Forward pass through the model for detoxification\n",
    "        detox_loss = torch.tensor(0.0, device=detox_input_ids.device)\n",
    "        if detox_labels is not None:\n",
    "            detox_outputs = model(input_ids=detox_input_ids, attention_mask=detox_attention_mask, labels=detox_labels)\n",
    "            detox_loss = detox_outputs.loss\n",
    "\n",
    "        classification_loss = torch.tensor(0.0, device=detox_input_ids.device)\n",
    "        if cls_labels is not None:\n",
    "            encoder_outputs = model.encoder(input_ids=cls_input_ids, attention_mask=cls_attention_mask, return_dict=True)\n",
    "            hidden_states = encoder_outputs.last_hidden_state\n",
    "\n",
    "            # Mean pooling over the sequence length\n",
    "            # pooled_states = hidden_states.mean(dim=1)  # [batch_size, d_model]\n",
    "            # print(pooled_states.shape)\n",
    "            classification_logits = model.classification_head(hidden_states)\n",
    "\n",
    "            print(classification_logits.shape, cls_labels.shape)\n",
    "            classification_loss = self.classification_loss_fn(classification_logits, cls_labels)\n",
    "\n",
    "        # Combine the losses (you can use a weighted sum if needed)\n",
    "        total_loss = seq2seq_loss + classification_loss\n",
    "\n",
    "        if return_outputs:\n",
    "            return total_loss, outputs\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "# Example usage\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./two_losses_results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=50,\n",
    "    logging_steps=20,\n",
    "    save_total_limit=3,\n",
    "    metric_for_best_model='loss',\n",
    "    greater_is_better=False,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    # data_collator=data_collator,\n",
    ")\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "4a1700e7-02e9-422d-b63e-f84ca36f27cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 128, 2]) torch.Size([8, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [8, 2], got [8, 128]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[322], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2529\u001b[0m )\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3675\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3674\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3675\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3677\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3679\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3680\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3681\u001b[0m ):\n",
      "Cell \u001b[0;32mIn[321], line 36\u001b[0m, in \u001b[0;36mCustomTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     classification_logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mclassification_head(hidden_states)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(classification_logits\u001b[38;5;241m.\u001b[39mshape, cls_labels\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 36\u001b[0m     classification_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassification_loss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassification_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Combine the losses (you can use a weighted sum if needed)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m seq2seq_loss \u001b[38;5;241m+\u001b[39m classification_loss\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [8, 2], got [8, 128]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eda761-7826-47c2-8f86-29e53f39986d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
