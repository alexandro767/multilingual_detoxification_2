{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "055b86ef-3f74-4584-b6fb-54c14f7c597d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, MT5ForConditionalGeneration, Trainer, TrainingArguments, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset, concatenate_datasets, ClassLabel, Dataset, DatasetDict\n",
    "import numpy as np\n",
    "import huggingface_hub\n",
    "from sklearn.metrics import f1_score\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "token = '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'\n",
    "huggingface_hub.login(token)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21d3e870-b2f6-4f50-bb69-b1241a24b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLossesModel(MT5ForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.classification_head = nn.Linear(config.d_model, 2)  # Assuming binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d8e0604-f712-40c9-920c-ac5903e6315e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of TwoLossesModel were not initialized from the model checkpoint at bigscience/mt0-large and are newly initialized: ['classification_head.bias', 'classification_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bigscience/mt0-large')\n",
    "\n",
    "model = TwoLossesModel.from_pretrained(\"bigscience/mt0-large\")\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c56447d-4aa5-4d8e-8f27-87221b3b5d88",
   "metadata": {},
   "source": [
    "## Уже не надо, но так создавался синтдетокс модифицированный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd2e25e0-16a9-4564-88f1-444d8eb80b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthdetoxm_raw = load_dataset('s-nlp/synthdetoxm')['train']#['ru']\n",
    "synthdetoxm = synthdetoxm_raw.filter(lambda x: x['lang']=='ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1501fca9-de5a-4476-8796-733afac4fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_lexicon = load_dataset('textdetox/multilingual_toxic_lexicon')['ru']\n",
    "toxic_lexicon_words = toxic_lexicon['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6024017c-5749-4c1a-a133-7e4b790f4508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['toxic_sentence', 'neutral_sentence', 'lang'],\n",
       "    num_rows: 4000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthdetoxm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "413c6d57-1665-4c39-841b-2fb9ef8556a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 140517\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e076e1c9-d249-464d-9662-71d1cf471de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_LEXICON(examples):\n",
    "    tokenized_input = tokenizer(examples['text'], padding='max_length', max_length=64, return_tensors=\"pt\")\n",
    "    \n",
    "    # word_tokens = tokenized_input.tokens()\n",
    "    word_ids = tokenized_input.tokens()\n",
    "\n",
    "    return {'input_ids': tokenized_input['input_ids'][0], 'word_tokens': word_ids}\n",
    "\n",
    "def leave_only_sentence_tokens_LEXICON(example):\n",
    "    tokens = example['input_ids']\n",
    "    word_tokens_old = example['word_tokens']\n",
    "    word_tokens = []\n",
    "    clean_tokens = []\n",
    "    for tok, wt in list(zip(tokens, word_tokens_old)):\n",
    "        # for i, t in enumerate(tok):\n",
    "        if tok == tokenizer.eos_token_id:\n",
    "            break\n",
    "        else:\n",
    "            clean_tokens.append(tok)\n",
    "            word_tokens.append(wt)\n",
    "    return {'clean_input_ids': clean_tokens, 'clean_word_tokens': word_tokens}\n",
    "\n",
    "def preprocess_SYNTHDETOX(examples):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    # print(examples, type(examples['toxic_sentence']))\n",
    "    # return\n",
    "    # for tox, neu, lang in list(zip(examples['toxic_sentence'], examples['neutral_sentence'], examples['lang'])):\n",
    "    # for i in range(len(examples['toxic_sentence'])):\n",
    "        # print(tox, neu, lang)\n",
    "    tox = examples['toxic_sentence']\n",
    "    neu = examples['neutral_sentence']\n",
    "    lang = examples['lang']\n",
    "    lang_map = {\n",
    "        'ru': 'russian'\n",
    "    }\n",
    "    if tox:  # If toxic text is not empty\n",
    "        prompt = f\"Detoxify this {lang_map[lang]} text and return answer also in {lang_map[lang]}. Here is the text: {tox}\"\n",
    "        # print(lang_map[lang])\n",
    "        # prompt = \"Detoxify this \" + lang_map[lang] + \" text and return answer also in \" + lang_map[lang] + \". Here is the text: \" + tox\n",
    "        inputs.append(prompt)\n",
    "        targets.append(neu)\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "    word_inputs = tokenizer(examples['toxic_sentence'], padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    # word_ids = word_inputs.word_ids()\n",
    "    word_tokens = word_inputs.tokens()\n",
    "    \n",
    "    # Tokenize target texts\n",
    "    labels = tokenizer(targets, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels\n",
    "\n",
    "    model_inputs['toxic_word_tokens'] = word_tokens\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "def find_overlap_position(main_array, sub_array):\n",
    "    \"\"\"\n",
    "    Find the first position where sub_array starts in main_array.\n",
    "    \n",
    "    Args:\n",
    "        main_array (list): The main array to search within.\n",
    "        sub_array (list): The sub_array to search for.\n",
    "    \n",
    "    Returns:\n",
    "        int: The starting index of the first occurrence of sub_array in main_array, or -1 if not found.\n",
    "    \"\"\"\n",
    "    main_len = len(main_array)\n",
    "    sub_len = len(sub_array)\n",
    "    \n",
    "    # If sub_array is longer than main_array, it cannot be contained within it\n",
    "    if sub_len > main_len:\n",
    "        return -1\n",
    "    \n",
    "    # Iterate through main_array to find sub_array\n",
    "    for i in range(main_len - sub_len + 1):\n",
    "        # Check if the slice of main_array matches sub_array\n",
    "        if main_array[i:i + sub_len] == sub_array:\n",
    "            return i\n",
    "    \n",
    "    # If no match is found, return -1\n",
    "    return -1\n",
    "\n",
    "def align_LEXICON(example):\n",
    "    input_tokens = example['toxic_word_tokens']\n",
    "    token_classes = [0] * 128\n",
    "    \n",
    "    for lexicon_tokens in toxic_lexicon['clean_word_tokens']:\n",
    "        res = find_overlap_position(input_tokens, lexicon_tokens)\n",
    "        if res != -1:\n",
    "            token_classes[res:res+len(lexicon_tokens)] = [1] * len(lexicon_tokens)\n",
    "            \n",
    "    return {'token_classes': token_classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "85fcdaad-2430-4fe2-b0ca-2e917e3c7de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b842689e33ee44e69d72e2cbac1cc288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/140517 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toxic_lexicon = toxic_lexicon.map(\n",
    "    tokenize_LEXICON, \n",
    "    batched=False\n",
    ")\n",
    "\n",
    "toxic_lexicon = toxic_lexicon.map(\n",
    "    leave_only_sentence_tokens_LEXICON, \n",
    "    batched=False\n",
    ")\n",
    "\n",
    "synthdetoxm = synthdetoxm.map(\n",
    "    preprocess_SYNTHDETOX, \n",
    "    batched=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "802cbd75-6713-47d8-83c2-ba428994aae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05088e872a94e8a97f948669b2d345c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "synthdetoxm = synthdetoxm.map(\n",
    "    align_LEXICON, \n",
    "    batched=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7d993bb9-0614-4a76-be1d-0352fa20f610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ae17f623c74bbd86b157a77be4d7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d1f2c53975745f7bbde3aaa70d2060e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/alexandro767/synthdetoxm_ru_with_token_classes/commit/5b79e4b577fa97bae2beb9f61cbef092f8ee76e2', commit_message='Upload dataset', commit_description='', oid='5b79e4b577fa97bae2beb9f61cbef092f8ee76e2', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/alexandro767/synthdetoxm_ru_with_token_classes', endpoint='https://huggingface.co', repo_type='dataset', repo_id='alexandro767/synthdetoxm_ru_with_token_classes'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# synthdetoxm[1]\n",
    "synthdetoxm.push_to_hub(repo_id='synthdetoxm_ru_with_token_classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28620be6-eb9a-45e3-97ee-e556b1465f4a",
   "metadata": {},
   "source": [
    "## Актуально"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "220239ad-7035-4027-9db0-dd1539ed9b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthdetoxm_new = load_dataset('alexandro767/synthdetoxm_ru_with_token_classes')['train']\n",
    "\n",
    "synthdetoxm_new.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ff0e564-2da7-4949-9e7c-c05b0d3ad646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_squeeze(examples):\n",
    "    input_ids = examples['input_ids'].squeeze()\n",
    "    attention_mask = examples['attention_mask'].squeeze()\n",
    "    labels = examples['labels'].squeeze()\n",
    "\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "synthdetoxm_new = synthdetoxm_new.map(go_squeeze, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d08be7b7-cc9e-4f1d-a98d-de30139d2a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 10\n",
    "\n",
    "df_for_collator = Dataset.from_dict({'toxic_sentence': synthdetoxm_new['toxic_sentence'][:top_n],\n",
    "                                     'neutral_sentence': synthdetoxm_new['neutral_sentence'][:top_n],\n",
    "                                     'lang': synthdetoxm_new['lang'][:top_n],\n",
    "                                     'cls_input_ids': synthdetoxm_new['input_ids'][:top_n], \n",
    "                                     'cls_attention_mask': synthdetoxm_new['attention_mask'][:top_n], \n",
    "                                     'cls_labels': synthdetoxm_new['token_classes'][:top_n], \n",
    "                                     'detox_input_ids': synthdetoxm_new['input_ids'][:top_n], \n",
    "                                     'detox_attention_mask': synthdetoxm_new['attention_mask'][:top_n],\n",
    "                                     'detox_labels': synthdetoxm_new['labels'][:top_n]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc2f6e71-0996-46eb-b5d9-0342b6eb60dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = df_for_collator.train_test_split(test_size=0.15, shuffle=True, seed=42)\n",
    "\n",
    "train_dataset = train_test['train']\n",
    "eval_dataset = train_test['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8dd6dd6f-1310-4a9a-8be6-6ff2d774ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c960dc7-550f-485e-a020-19266e101767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Define your classification loss\n",
    "        self.classification_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # Unpack inputs\n",
    "        if model.training:\n",
    "            cls_input_ids = inputs.get(\"cls_input_ids\")\n",
    "            cls_attention_mask = inputs.get(\"cls_attention_mask\")\n",
    "            cls_labels = inputs.get(\"cls_labels\")\n",
    "            detox_input_ids = inputs.get(\"detox_input_ids\")\n",
    "            detox_attention_mask = inputs.get(\"detox_attention_mask\")\n",
    "            detox_labels = inputs.get(\"detox_labels\")\n",
    "    \n",
    "            # Forward pass through the model for detoxification\n",
    "            detox_loss = torch.tensor(0.0, device=detox_input_ids.device)\n",
    "            detox_outputs = None\n",
    "            if detox_labels is not None:\n",
    "                detox_outputs = model(input_ids=detox_input_ids, attention_mask=detox_attention_mask, labels=detox_labels)\n",
    "                detox_loss = detox_outputs.loss\n",
    "                detox_loss = torch.mean(detox_loss, axis=0)\n",
    "    \n",
    "            classification_loss = torch.tensor(0.0, device=detox_input_ids.device)\n",
    "            if cls_labels is not None:\n",
    "                encoder_outputs = model.module.encoder(input_ids=cls_input_ids, attention_mask=cls_attention_mask, return_dict=True)\n",
    "                hidden_states = encoder_outputs.last_hidden_state\n",
    "    \n",
    "                # Mean pooling over the sequence length\n",
    "                classification_logits = model.module.classification_head(hidden_states)\n",
    "                classification_preds = torch.argmax(classification_logits, dim=2).to(torch.float64)\n",
    "    \n",
    "                classification_loss = self.classification_loss_fn(classification_preds, cls_labels.to(torch.float64))\n",
    "    \n",
    "            # weight = 0.01\n",
    "            # total_loss = (1.-weight) * detox_loss + weight * classification_loss\n",
    "            print(detox_loss, classification_loss)\n",
    "            total_loss = detox_loss + classification_loss\n",
    "            # print(total_loss)\n",
    "\n",
    "            if return_outputs:\n",
    "                return total_loss, detox_outputs\n",
    "\n",
    "            return total_loss\n",
    "            \n",
    "        else:\n",
    "            detox_input_ids = inputs.get(\"input_ids\")\n",
    "            detox_attention_mask = inputs.get(\"attention_mask\")\n",
    "            detox_labels = inputs.get(\"labels\")\n",
    "            \n",
    "            # Forward pass through the model for detoxification\n",
    "            detox_loss = torch.tensor(0.0, device=detox_input_ids.device)\n",
    "            detox_outputs = None\n",
    "            \n",
    "            if detox_labels is not None:\n",
    "                detox_outputs = model(input_ids=detox_input_ids, attention_mask=detox_attention_mask, labels=detox_labels)\n",
    "                detox_loss = detox_outputs.loss\n",
    "                detox_loss = torch.mean(detox_loss, axis=0)\n",
    "                # print('eval_loss', detox_loss)\n",
    "\n",
    "            if return_outputs:\n",
    "                return detox_loss, detox_outputs\n",
    "            \n",
    "            return detox_loss\n",
    "\n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        ds = self.eval_dataset\n",
    "        ds = ds.select_columns(['detox_input_ids', 'detox_attention_mask', 'detox_labels'])\n",
    "        ds = ds.rename_columns({'detox_input_ids': 'input_ids', 'detox_attention_mask': 'attention_mask', 'detox_labels': 'labels'})\n",
    "        return super().evaluate(ds, ignore_keys=kwargs['ignore_keys'])\n",
    "\n",
    "# Example usage\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./two_losses_mt0_large_7e_synthdetoxm_ru\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    # disable_tqdm=True,\n",
    "    learning_rate=9e-5,\n",
    "    optim='adafactor',\n",
    "    lr_scheduler_type='cosine',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=7,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=50,\n",
    "    logging_steps=30,\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model='loss',\n",
    "    greater_is_better=False,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    "    gradient_accumulation_steps=64,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    # data_collator=data_collator,\n",
    ")\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a1700e7-02e9-422d-b63e-f84ca36f27cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19.4816, device='cuda:0', grad_fn=<MeanBackward1>) tensor(27.8992, device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:17, Epoch 6/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>21.816549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>21.742596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>21.340385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>21.072069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>20.696497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>20.260742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19.4399, device='cuda:0', grad_fn=<MeanBackward1>) tensor(27.8992, device='cuda:0', dtype=torch.float64)\n",
      "tensor(19.9365, device='cuda:0', grad_fn=<MeanBackward1>) tensor(27.8992, device='cuda:0', dtype=torch.float64)\n",
      "tensor(19.3698, device='cuda:0', grad_fn=<MeanBackward1>) tensor(27.8992, device='cuda:0', dtype=torch.float64)\n",
      "tensor(19.0340, device='cuda:0', grad_fn=<MeanBackward1>) tensor(27.8992, device='cuda:0', dtype=torch.float64)\n",
      "tensor(18.5990, device='cuda:0', grad_fn=<MeanBackward1>) tensor(27.8992, device='cuda:0', dtype=torch.float64)\n",
      "tensor(17.9710, device='cuda:0', grad_fn=<MeanBackward1>) tensor(27.8992, device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 955.25 MiB is free. Process 1435931 has 3.13 GiB memory in use. Process 1435903 has 19.61 GiB memory in use. Of the allocated memory 17.97 GiB is allocated by PyTorch, and 405.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2529\u001b[0m )\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3712\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_accepts_loss_kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3710\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m-> 3712\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3714\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2246\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2246\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py:289\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:34\u001b[0m, in \u001b[0;36mBroadcast.backward\u001b[0;34m(ctx, *grad_outputs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(ctx, \u001b[38;5;241m*\u001b[39mgrad_outputs):\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m+\u001b[39m \u001b[43mReduceAddCoalesced\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:45\u001b[0m, in \u001b[0;36mReduceAddCoalesced.forward\u001b[0;34m(ctx, destination, num_inputs, *grads)\u001b[0m\n\u001b[1;32m     41\u001b[0m ctx\u001b[38;5;241m.\u001b[39mtarget_gpus \u001b[38;5;241m=\u001b[39m [grads[i]\u001b[38;5;241m.\u001b[39mget_device() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(grads), num_inputs)]\n\u001b[1;32m     43\u001b[0m grads_ \u001b[38;5;241m=\u001b[39m [grads[i:i \u001b[38;5;241m+\u001b[39m num_inputs]\n\u001b[1;32m     44\u001b[0m           \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(grads), num_inputs)]\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_add_coalesced\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/comm.py:141\u001b[0m, in \u001b[0;36mreduce_add_coalesced\u001b[0;34m(inputs, destination, buffer_size)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunks \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mitrs):\n\u001b[1;32m    140\u001b[0m     flat_tensors \u001b[38;5;241m=\u001b[39m [_flatten_dense_tensors(chunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]  \u001b[38;5;66;03m# (num_gpus,)\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     flat_result \u001b[38;5;241m=\u001b[39m \u001b[43mreduce_add\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m _unflatten_dense_tensors(flat_result, chunks[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;66;03m# The unflattened tensors do not share storage, and we don't expose\u001b[39;00m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;66;03m# base flat tensor anyways, so give them different version counters.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;66;03m# See NOTE [ Version Counter in comm.*_coalesced ]\u001b[39;00m\n\u001b[1;32m    146\u001b[0m         output\u001b[38;5;241m.\u001b[39mappend(t\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/comm.py:93\u001b[0m, in \u001b[0;36mreduce_add\u001b[0;34m(inputs, destination)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nccl\u001b[38;5;241m.\u001b[39mis_available(inputs):\n\u001b[0;32m---> 93\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mroot_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     nccl\u001b[38;5;241m.\u001b[39mreduce(inputs, output\u001b[38;5;241m=\u001b[39mresult, root\u001b[38;5;241m=\u001b[39mroot_index)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 955.25 MiB is free. Process 1435931 has 3.13 GiB memory in use. Process 1435903 has 19.61 GiB memory in use. Of the allocated memory 17.97 GiB is allocated by PyTorch, and 405.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f0d6313-77a8-4bec-81b9-c8c70c980f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./two_losses_model/eval_toxic_mt0_large_only_ru_w1.csv')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4ff9783-f494-45b3-97a4-99ffca6dbe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './two_losses_mt0_large_7e_synthdetoxm_ru/checkpoint-42'\n",
    "\n",
    "# Load the model from the checkpoint\n",
    "model_cpt = TwoLossesModel.from_pretrained(checkpoint_path).to(device)\n",
    "\n",
    "# Load the tokenizer from the checkpoint\n",
    "tokenizer_cpt = AutoTokenizer.from_pretrained('bigscience/mt0-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cb8f6c5-6a2c-44ee-97af-bc1b6d4063b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input text\n",
    "eval_dataset.set_format(type='torch', columns=['detox_input_ids', 'detox_attention_mask', 'detox_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c029999-b028-47b3-a99c-1c3b248ab4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91bcda35-7ee5-4e10-a343-433d5cfb7cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detoxified text\n",
    "eval_bs = 4\n",
    "detoxified_sentence = []\n",
    "\n",
    "for i in range(0, len(eval_dataset), eval_bs):\n",
    "    batch = eval_dataset[i:i + eval_bs]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_cpt.generate(input_ids=batch['detox_input_ids'].to(device), attention_mask=batch['detox_attention_mask'].to(device), max_new_tokens=128)\n",
    "    \n",
    "    detoxified_outputs = tokenizer_cpt.batch_decode(outputs, max_lenght=128, skip_special_tokens=True)\n",
    "\n",
    "    for do in detoxified_outputs:\n",
    "        detoxified_sentence.append(do)\n",
    "\n",
    "toxic_sentence = eval_dataset['toxic_sentence']\n",
    "lang = eval_dataset['lang']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a632fc7-279e-455c-ac1e-0592be005690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detoxified_sentence[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80c4a36f-f089-4f5e-b91f-3f5cf86f3718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(detoxified_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f92b911-5798-4057-a5b3-e8747caf9a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(toxic, neutral, lang):\n",
    "    assert len(toxic) == len(neutral) == len(lang)\n",
    "    toxic_dataset = pd.DataFrame(data=np.array([toxic, neutral, lang]).T, columns=['toxic_sentence', 'neutral_sentence', 'lang'])\n",
    "    neutral_dataset = pd.DataFrame(data=np.array([neutral, lang]).T, columns=['neutral_sentence', 'lang'])\n",
    "    return toxic_dataset, neutral_dataset\n",
    "\n",
    "toxic_dataset, neutral_dataset = make_dataset(toxic_sentence, detoxified_sentence, lang)\n",
    "toxic_dataset.to_csv('./two_losses_model/eval_toxic_mt0_large_only_ru_w1.csv', index=None)\n",
    "neutral_dataset.to_csv('./two_losses_model/eval_neutral_mt0_large_only_ru_w1.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
