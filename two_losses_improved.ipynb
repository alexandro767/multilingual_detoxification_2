{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "055b86ef-3f74-4584-b6fb-54c14f7c597d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, MT5ForConditionalGeneration, Trainer, TrainingArguments, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset, concatenate_datasets, ClassLabel, Dataset, DatasetDict\n",
    "import numpy as np\n",
    "import huggingface_hub\n",
    "from sklearn.metrics import f1_score\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "token = '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'\n",
    "huggingface_hub.login(token)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21d3e870-b2f6-4f50-bb69-b1241a24b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLossesModel(MT5ForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.classification_head = nn.Linear(config.d_model, 2)  # Assuming binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d8e0604-f712-40c9-920c-ac5903e6315e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of TwoLossesModel were not initialized from the model checkpoint at bigscience/mt0-base and are newly initialized: ['classification_head.bias', 'classification_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bigscience/mt0-base')\n",
    "\n",
    "model = TwoLossesModel.from_pretrained(\"bigscience/mt0-base\")\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c56447d-4aa5-4d8e-8f27-87221b3b5d88",
   "metadata": {},
   "source": [
    "## Уже не надо, но так создавался синтдетокс модифицированный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd2e25e0-16a9-4564-88f1-444d8eb80b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthdetoxm_raw = load_dataset('s-nlp/synthdetoxm')['train']#['ru']\n",
    "synthdetoxm = synthdetoxm_raw.filter(lambda x: x['lang']=='ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1501fca9-de5a-4476-8796-733afac4fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_lexicon = load_dataset('textdetox/multilingual_toxic_lexicon')['ru']\n",
    "toxic_lexicon_words = toxic_lexicon['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6024017c-5749-4c1a-a133-7e4b790f4508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['toxic_sentence', 'neutral_sentence', 'lang'],\n",
       "    num_rows: 4000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthdetoxm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "413c6d57-1665-4c39-841b-2fb9ef8556a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 140517\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e076e1c9-d249-464d-9662-71d1cf471de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_LEXICON(examples):\n",
    "    tokenized_input = tokenizer(examples['text'], padding='max_length', max_length=64, return_tensors=\"pt\")\n",
    "    \n",
    "    # word_tokens = tokenized_input.tokens()\n",
    "    word_ids = tokenized_input.tokens()\n",
    "\n",
    "    return {'input_ids': tokenized_input['input_ids'][0], 'word_tokens': word_ids}\n",
    "\n",
    "def leave_only_sentence_tokens_LEXICON(example):\n",
    "    tokens = example['input_ids']\n",
    "    word_tokens_old = example['word_tokens']\n",
    "    word_tokens = []\n",
    "    clean_tokens = []\n",
    "    for tok, wt in list(zip(tokens, word_tokens_old)):\n",
    "        # for i, t in enumerate(tok):\n",
    "        if tok == tokenizer.eos_token_id:\n",
    "            break\n",
    "        else:\n",
    "            clean_tokens.append(tok)\n",
    "            word_tokens.append(wt)\n",
    "    return {'clean_input_ids': clean_tokens, 'clean_word_tokens': word_tokens}\n",
    "\n",
    "def preprocess_SYNTHDETOX(examples):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    # print(examples, type(examples['toxic_sentence']))\n",
    "    # return\n",
    "    # for tox, neu, lang in list(zip(examples['toxic_sentence'], examples['neutral_sentence'], examples['lang'])):\n",
    "    # for i in range(len(examples['toxic_sentence'])):\n",
    "        # print(tox, neu, lang)\n",
    "    tox = examples['toxic_sentence']\n",
    "    neu = examples['neutral_sentence']\n",
    "    lang = examples['lang']\n",
    "    lang_map = {\n",
    "        'ru': 'russian'\n",
    "    }\n",
    "    if tox:  # If toxic text is not empty\n",
    "        prompt = f\"Detoxify this {lang_map[lang]} text and return answer also in {lang_map[lang]}. Here is the text: {tox}\"\n",
    "        # print(lang_map[lang])\n",
    "        # prompt = \"Detoxify this \" + lang_map[lang] + \" text and return answer also in \" + lang_map[lang] + \". Here is the text: \" + tox\n",
    "        inputs.append(prompt)\n",
    "        targets.append(neu)\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "    word_inputs = tokenizer(examples['toxic_sentence'], padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    # word_ids = word_inputs.word_ids()\n",
    "    word_tokens = word_inputs.tokens()\n",
    "    \n",
    "    # Tokenize target texts\n",
    "    labels = tokenizer(targets, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels\n",
    "\n",
    "    model_inputs['toxic_word_tokens'] = word_tokens\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "def find_overlap_position(main_array, sub_array):\n",
    "    \"\"\"\n",
    "    Find the first position where sub_array starts in main_array.\n",
    "    \n",
    "    Args:\n",
    "        main_array (list): The main array to search within.\n",
    "        sub_array (list): The sub_array to search for.\n",
    "    \n",
    "    Returns:\n",
    "        int: The starting index of the first occurrence of sub_array in main_array, or -1 if not found.\n",
    "    \"\"\"\n",
    "    main_len = len(main_array)\n",
    "    sub_len = len(sub_array)\n",
    "    \n",
    "    # If sub_array is longer than main_array, it cannot be contained within it\n",
    "    if sub_len > main_len:\n",
    "        return -1\n",
    "    \n",
    "    # Iterate through main_array to find sub_array\n",
    "    for i in range(main_len - sub_len + 1):\n",
    "        # Check if the slice of main_array matches sub_array\n",
    "        if main_array[i:i + sub_len] == sub_array:\n",
    "            return i\n",
    "    \n",
    "    # If no match is found, return -1\n",
    "    return -1\n",
    "\n",
    "def align_LEXICON(example):\n",
    "    input_tokens = example['toxic_word_tokens']\n",
    "    token_classes = [0] * 128\n",
    "    \n",
    "    for lexicon_tokens in toxic_lexicon['clean_word_tokens']:\n",
    "        res = find_overlap_position(input_tokens, lexicon_tokens)\n",
    "        if res != -1:\n",
    "            token_classes[res:res+len(lexicon_tokens)] = [1] * len(lexicon_tokens)\n",
    "            \n",
    "    return {'token_classes': token_classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "85fcdaad-2430-4fe2-b0ca-2e917e3c7de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b842689e33ee44e69d72e2cbac1cc288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/140517 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toxic_lexicon = toxic_lexicon.map(\n",
    "    tokenize_LEXICON, \n",
    "    batched=False\n",
    ")\n",
    "\n",
    "toxic_lexicon = toxic_lexicon.map(\n",
    "    leave_only_sentence_tokens_LEXICON, \n",
    "    batched=False\n",
    ")\n",
    "\n",
    "synthdetoxm = synthdetoxm.map(\n",
    "    preprocess_SYNTHDETOX, \n",
    "    batched=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "802cbd75-6713-47d8-83c2-ba428994aae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05088e872a94e8a97f948669b2d345c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "synthdetoxm = synthdetoxm.map(\n",
    "    align_LEXICON, \n",
    "    batched=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7d993bb9-0614-4a76-be1d-0352fa20f610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ae17f623c74bbd86b157a77be4d7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d1f2c53975745f7bbde3aaa70d2060e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/alexandro767/synthdetoxm_ru_with_token_classes/commit/5b79e4b577fa97bae2beb9f61cbef092f8ee76e2', commit_message='Upload dataset', commit_description='', oid='5b79e4b577fa97bae2beb9f61cbef092f8ee76e2', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/alexandro767/synthdetoxm_ru_with_token_classes', endpoint='https://huggingface.co', repo_type='dataset', repo_id='alexandro767/synthdetoxm_ru_with_token_classes'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# synthdetoxm[1]\n",
    "synthdetoxm.push_to_hub(repo_id='synthdetoxm_ru_with_token_classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28620be6-eb9a-45e3-97ee-e556b1465f4a",
   "metadata": {},
   "source": [
    "## Актуально"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "220239ad-7035-4027-9db0-dd1539ed9b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthdetoxm_new = load_dataset('alexandro767/synthdetoxm_ru_with_token_classes')['train']\n",
    "\n",
    "synthdetoxm_new.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ff0e564-2da7-4949-9e7c-c05b0d3ad646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_squeeze(examples):\n",
    "    input_ids = examples['input_ids'].squeeze()\n",
    "    attention_mask = examples['attention_mask'].squeeze()\n",
    "    labels = examples['labels'].squeeze()\n",
    "\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "synthdetoxm_new = synthdetoxm_new.map(go_squeeze, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d08be7b7-cc9e-4f1d-a98d-de30139d2a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 40\n",
    "\n",
    "df_for_collator = Dataset.from_dict({'toxic_sentence': synthdetoxm_new['toxic_sentence'][:top_n],\n",
    "                                     'neutral_sentence': synthdetoxm_new['neutral_sentence'][:top_n],\n",
    "                                     'lang': synthdetoxm_new['lang'][:top_n],\n",
    "                                     'cls_input_ids': synthdetoxm_new['input_ids'][:top_n], \n",
    "                                     'cls_attention_mask': synthdetoxm_new['attention_mask'][:top_n], \n",
    "                                     'cls_labels': synthdetoxm_new['token_classes'][:top_n], \n",
    "                                     'detox_input_ids': synthdetoxm_new['input_ids'][:top_n], \n",
    "                                     'detox_attention_mask': synthdetoxm_new['attention_mask'][:top_n],\n",
    "                                     'detox_labels': synthdetoxm_new['labels'][:top_n]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc2f6e71-0996-46eb-b5d9-0342b6eb60dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = df_for_collator.train_test_split(test_size=0.15, shuffle=True, seed=42)\n",
    "\n",
    "train_dataset = train_test['train']\n",
    "eval_dataset = train_test['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dd6dd6f-1310-4a9a-8be6-6ff2d774ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c960dc7-550f-485e-a020-19266e101767",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_pred_list = []\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.classification_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # Unpack inputs\n",
    "        if model.training:\n",
    "            cls_input_ids = inputs.get(\"cls_input_ids\")\n",
    "            cls_attention_mask = inputs.get(\"cls_attention_mask\")\n",
    "            cls_labels = inputs.get(\"cls_labels\")\n",
    "            detox_input_ids = inputs.get(\"detox_input_ids\")\n",
    "            detox_attention_mask = inputs.get(\"detox_attention_mask\")\n",
    "            detox_labels = inputs.get(\"detox_labels\")\n",
    "    \n",
    "            # Forward pass through the model for detoxification\n",
    "            detox_loss = torch.tensor(0.0, device=detox_input_ids.device)\n",
    "            detox_outputs = None\n",
    "            if detox_labels is not None:\n",
    "                detox_outputs = model(input_ids=detox_input_ids, attention_mask=detox_attention_mask, labels=detox_labels)\n",
    "                detox_loss = detox_outputs.loss\n",
    "                detox_loss = torch.mean(detox_loss, axis=0)\n",
    "\n",
    "            \n",
    "            classification_loss = torch.tensor(0.0, device=detox_input_ids.device)\n",
    "            if cls_labels is not None:\n",
    "                encoder_outputs = model.module.encoder(input_ids=cls_input_ids, attention_mask=cls_attention_mask, return_dict=True)\n",
    "                hidden_states = encoder_outputs.last_hidden_state\n",
    "    \n",
    "                # Mean pooling over the sequence length\n",
    "                classification_logits = model.module.classification_head(hidden_states)\n",
    "                classification_preds = classification_logits.permute(0, 2, 1)\n",
    "                cls_pred_list.append(classification_preds)\n",
    "    \n",
    "                classification_loss = self.classification_loss_fn(classification_preds, cls_labels)\n",
    "    \n",
    "            # weight = 0.01\n",
    "            # total_loss = (1.-weight) * detox_loss + weight * classification_loss\n",
    "            # total_loss = detox_loss + classification_loss\n",
    "            print(detox_loss, classification_loss)\n",
    "            total_loss = classification_loss\n",
    "\n",
    "            if return_outputs:\n",
    "                return total_loss, detox_outputs\n",
    "\n",
    "            return total_loss\n",
    "            \n",
    "        else:\n",
    "            detox_input_ids = inputs.get(\"input_ids\")\n",
    "            detox_attention_mask = inputs.get(\"attention_mask\")\n",
    "            detox_labels = inputs.get(\"labels\")\n",
    "            \n",
    "            # Forward pass through the model for detoxification\n",
    "            detox_loss = torch.tensor(0.0, device=detox_input_ids.device)\n",
    "            detox_outputs = None\n",
    "            \n",
    "            if detox_labels is not None:\n",
    "                detox_outputs = model(input_ids=detox_input_ids, attention_mask=detox_attention_mask, labels=detox_labels)\n",
    "                detox_loss = detox_outputs.loss\n",
    "                detox_loss = torch.mean(detox_loss, axis=0)\n",
    "                # print('eval_loss', detox_loss)\n",
    "\n",
    "            if return_outputs:\n",
    "                return detox_loss, detox_outputs\n",
    "            \n",
    "            return detox_loss\n",
    "\n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        ds = self.eval_dataset\n",
    "        ds = ds.select_columns(['detox_input_ids', 'detox_attention_mask', 'detox_labels'])\n",
    "        ds = ds.rename_columns({'detox_input_ids': 'input_ids', 'detox_attention_mask': 'attention_mask', 'detox_labels': 'labels'})\n",
    "        return super().evaluate(ds, ignore_keys=kwargs['ignore_keys'])\n",
    "\n",
    "# Example usage\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./two_losses_model/does_the_model_classify\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    # disable_tqdm=True,\n",
    "    learning_rate=9e-5,\n",
    "    optim='adafactor',\n",
    "    lr_scheduler_type='cosine',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=50,\n",
    "    logging_steps=30,\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model='loss',\n",
    "    greater_is_better=False,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    "    gradient_accumulation_steps=64,\n",
    "    disable_tqdm=True,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    # data_collator=data_collator,\n",
    ")\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a1700e7-02e9-422d-b63e-f84ca36f27cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20.8654, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.5688e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(22.1002, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.6196e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(19.5398, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.2450e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(22.1090, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.9315e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(22.1236, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.1087e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "{'eval_loss': 21.93085479736328, 'eval_runtime': 0.5187, 'eval_samples_per_second': 11.568, 'eval_steps_per_second': 1.928, 'epoch': 1.0}\n",
      "tensor(20.5216, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.7897e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(22.5270, device='cuda:0', grad_fn=<MeanBackward1>) tensor(1.0577e+35, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(22.1802, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.1766e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(21.4729, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.1895e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(17.5064, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.3724e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "{'eval_loss': 21.93085479736328, 'eval_runtime': 0.5174, 'eval_samples_per_second': 11.596, 'eval_steps_per_second': 1.933, 'epoch': 2.0}\n",
      "tensor(23.2532, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.8207e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(19.6496, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.0743e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(20.9706, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.7695e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(20.7013, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.5399e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(24.3515, device='cuda:0', grad_fn=<MeanBackward1>) tensor(1.0202e+35, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "{'eval_loss': 21.930856704711914, 'eval_runtime': 0.5176, 'eval_samples_per_second': 11.591, 'eval_steps_per_second': 1.932, 'epoch': 3.0}\n",
      "tensor(22.9693, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.5253e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(21.4246, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.0524e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(19.1431, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.2376e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(20.7618, device='cuda:0', grad_fn=<MeanBackward1>) tensor(1.0227e+35, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(22.7008, device='cuda:0', grad_fn=<MeanBackward1>) tensor(1.0537e+35, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "{'eval_loss': 21.930856704711914, 'eval_runtime': 0.5189, 'eval_samples_per_second': 11.563, 'eval_steps_per_second': 1.927, 'epoch': 4.0}\n",
      "tensor(20.4349, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.0232e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(20.3128, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.7260e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(20.3657, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.1118e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(23.5799, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.9245e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(22.0297, device='cuda:0', grad_fn=<MeanBackward1>) tensor(1.1689e+35, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "{'eval_loss': 21.93085479736328, 'eval_runtime': 0.5212, 'eval_samples_per_second': 11.513, 'eval_steps_per_second': 1.919, 'epoch': 5.0}\n",
      "tensor(22.4383, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.7406e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(19.9186, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.9315e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(20.0852, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.9578e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(22.3982, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.3853e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(24.1720, device='cuda:0', grad_fn=<MeanBackward1>) tensor(6.8578e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "{'eval_loss': 21.93085479736328, 'eval_runtime': 0.5177, 'eval_samples_per_second': 11.589, 'eval_steps_per_second': 1.932, 'epoch': 6.0}\n",
      "tensor(20.9587, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.9644e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(20.2537, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.0494e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(22.4231, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.5001e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(22.0594, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.6436e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(19.6401, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.8618e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "{'eval_loss': 21.93085479736328, 'eval_runtime': 0.5176, 'eval_samples_per_second': 11.593, 'eval_steps_per_second': 1.932, 'epoch': 7.0}\n",
      "tensor(21.6793, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.0205e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(21.7357, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.3567e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(20.1543, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.9660e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(21.5497, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.8865e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(21.5845, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.4645e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "{'eval_loss': 21.930856704711914, 'eval_runtime': 0.5174, 'eval_samples_per_second': 11.597, 'eval_steps_per_second': 1.933, 'epoch': 8.0}\n",
      "tensor(22.7090, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.8584e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(19.8066, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.7232e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(20.4281, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.5018e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(23.5027, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.5614e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(15.6345, device='cuda:0', grad_fn=<MeanBackward1>) tensor(6.4235e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "{'eval_loss': 21.93085479736328, 'eval_runtime': 0.5177, 'eval_samples_per_second': 11.59, 'eval_steps_per_second': 1.932, 'epoch': 9.0}\n",
      "tensor(20.7498, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.0048e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(22.3689, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.9935e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(19.7944, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.6841e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(20.4141, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.7013e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(24.4115, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.5818e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "{'eval_loss': 21.93085289001465, 'eval_runtime': 0.5175, 'eval_samples_per_second': 11.595, 'eval_steps_per_second': 1.933, 'epoch': 10.0}\n",
      "tensor(22.2807, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.1208e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(18.5290, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.6436e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(22.9056, device='cuda:0', grad_fn=<MeanBackward1>) tensor(1.0239e+35, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(21.2946, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.5712e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(18.1782, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.2570e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "{'eval_loss': 21.930849075317383, 'eval_runtime': 0.5174, 'eval_samples_per_second': 11.597, 'eval_steps_per_second': 1.933, 'epoch': 11.0}\n",
      "tensor(21.1336, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.8255e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(21.4932, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.3724e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(20.9496, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.2683e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(21.3222, device='cuda:0', grad_fn=<MeanBackward1>) tensor(1.0221e+35, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(19.4824, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.2887e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "{'eval_loss': 21.930849075317383, 'eval_runtime': 0.5172, 'eval_samples_per_second': 11.602, 'eval_steps_per_second': 1.934, 'epoch': 12.0}\n",
      "tensor(22.7005, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.3720e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(20.1809, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.4181e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(20.8511, device='cuda:0', grad_fn=<MeanBackward1>) tensor(7.6099e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(21.2967, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.9918e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(21.3121, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.9884e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "{'eval_loss': 21.930849075317383, 'eval_runtime': 0.5187, 'eval_samples_per_second': 11.567, 'eval_steps_per_second': 1.928, 'epoch': 13.0}\n",
      "tensor(22.8754, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.9736e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(21.6164, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.0350e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(20.2826, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.7350e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(20.0369, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.8237e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(21.1242, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.6950e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "{'eval_loss': 21.93084716796875, 'eval_runtime': 0.5193, 'eval_samples_per_second': 11.554, 'eval_steps_per_second': 1.926, 'epoch': 14.0}\n",
      "tensor(19.7764, device='cuda:0', grad_fn=<MeanBackward1>) tensor(1.0331e+35, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(21.0874, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.7235e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(21.3223, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.1486e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(22.2979, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.6181e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(19.2392, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.7191e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "{'eval_loss': 21.93084716796875, 'eval_runtime': 0.5184, 'eval_samples_per_second': 11.574, 'eval_steps_per_second': 1.929, 'epoch': 15.0}\n",
      "tensor(21.5915, device='cuda:0', grad_fn=<MeanBackward1>) tensor(7.8303e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(20.0492, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.2886e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(23.0061, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.4160e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(19.3733, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.1859e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(23.0294, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.8889e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "{'eval_loss': 21.930845260620117, 'eval_runtime': 0.5183, 'eval_samples_per_second': 11.575, 'eval_steps_per_second': 1.929, 'epoch': 16.0}\n",
      "tensor(19.3214, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.5594e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(21.7068, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.5341e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(22.8976, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.8247e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(21.0531, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.5674e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(20.1368, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.0673e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "{'eval_loss': 21.930845260620117, 'eval_runtime': 0.5172, 'eval_samples_per_second': 11.602, 'eval_steps_per_second': 1.934, 'epoch': 17.0}\n",
      "tensor(19.9917, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.7614e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(20.6247, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.0718e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(21.6934, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.3789e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(22.0256, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.8705e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(22.4004, device='cuda:0', grad_fn=<MeanBackward1>) tensor(1.0378e+35, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "{'eval_loss': 21.93084144592285, 'eval_runtime': 0.5174, 'eval_samples_per_second': 11.596, 'eval_steps_per_second': 1.933, 'epoch': 18.0}\n",
      "tensor(21.3982, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.3658e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(21.5473, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.7198e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(20.9997, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.6749e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(20.0459, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.3637e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(23.6491, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.5952e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "{'eval_loss': 21.93083953857422, 'eval_runtime': 0.5193, 'eval_samples_per_second': 11.554, 'eval_steps_per_second': 1.926, 'epoch': 19.0}\n",
      "tensor(21.3183, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.5344e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(21.5757, device='cuda:0', grad_fn=<MeanBackward1>) tensor(9.1045e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(21.8450, device='cuda:0', grad_fn=<MeanBackward1>) tensor(8.9939e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(21.1406, device='cuda:0', grad_fn=<MeanBackward1>) tensor(1.0107e+35, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "tensor(18.3063, device='cuda:0', grad_fn=<MeanBackward1>) tensor(7.5068e+34, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
      "{'eval_loss': 21.93083381652832, 'eval_runtime': 0.5181, 'eval_samples_per_second': 11.581, 'eval_steps_per_second': 1.93, 'epoch': 20.0}\n",
      "{'train_runtime': 80.8138, 'train_samples_per_second': 8.414, 'train_steps_per_second': 0.247, 'train_loss': 7.173730315662583e+33, 'epoch': 20.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=7.173730315662583e+33, metrics={'train_runtime': 80.8138, 'train_samples_per_second': 8.414, 'train_steps_per_second': 0.247, 'train_loss': 7.173730315662583e+33, 'epoch': 20.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f0d6313-77a8-4bec-81b9-c8c70c980f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4ff9783-f494-45b3-97a4-99ffca6dbe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './two_losses_mt0_large_7e_synthdetoxm_ru/checkpoint-42'\n",
    "\n",
    "# Load the model from the checkpoint\n",
    "model_cpt = TwoLossesModel.from_pretrained(checkpoint_path).to(device)\n",
    "\n",
    "# Load the tokenizer from the checkpoint\n",
    "tokenizer_cpt = AutoTokenizer.from_pretrained('bigscience/mt0-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cb8f6c5-6a2c-44ee-97af-bc1b6d4063b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input text\n",
    "eval_dataset.set_format(type='torch', columns=['detox_input_ids', 'detox_attention_mask', 'detox_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91bcda35-7ee5-4e10-a343-433d5cfb7cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detoxified text\n",
    "eval_bs = 4\n",
    "detoxified_sentence = []\n",
    "\n",
    "for i in range(0, len(eval_dataset), eval_bs):\n",
    "    batch = eval_dataset[i:i + eval_bs]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_cpt.generate(input_ids=batch['detox_input_ids'].to(device), attention_mask=batch['detox_attention_mask'].to(device), max_new_tokens=128)\n",
    "    \n",
    "    detoxified_outputs = tokenizer_cpt.batch_decode(outputs, max_lenght=128, skip_special_tokens=True)\n",
    "\n",
    "    for do in detoxified_outputs:\n",
    "        detoxified_sentence.append(do)\n",
    "\n",
    "toxic_sentence = eval_dataset['toxic_sentence']\n",
    "lang = eval_dataset['lang']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a632fc7-279e-455c-ac1e-0592be005690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detoxified_sentence[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80c4a36f-f089-4f5e-b91f-3f5cf86f3718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(detoxified_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f92b911-5798-4057-a5b3-e8747caf9a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(toxic, neutral, lang):\n",
    "    assert len(toxic) == len(neutral) == len(lang)\n",
    "    toxic_dataset = pd.DataFrame(data=np.array([toxic, neutral, lang]).T, columns=['toxic_sentence', 'neutral_sentence', 'lang'])\n",
    "    neutral_dataset = pd.DataFrame(data=np.array([neutral, lang]).T, columns=['neutral_sentence', 'lang'])\n",
    "    return toxic_dataset, neutral_dataset\n",
    "\n",
    "toxic_dataset, neutral_dataset = make_dataset(toxic_sentence, detoxified_sentence, lang)\n",
    "toxic_dataset.to_csv('./two_losses_model/eval_toxic_mt0_large_only_ru_w1.csv', index=None)\n",
    "neutral_dataset.to_csv('./two_losses_model/eval_neutral_mt0_large_only_ru_w1.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
